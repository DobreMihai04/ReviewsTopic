{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5708d93f-e0ca-49fe-9067-c308534410bf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession, Window\n",
    "from pyspark.sql.functions import col, avg, count, date_trunc, expr, concat_ws, to_date, lag\n",
    "from pyspark.sql.types import StringType, StructField, StructType\n",
    "import logging\n",
    "\n",
    "# Initialize logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Set export path\n",
    "EXPORT_PATH_BASE = \"s3a://topic-prediction/dashboard_data\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "427187e1-7d6e-4322-9996-93d4c05ccb92",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Define functions\n",
    "\n",
    "# Function to load and validate data\n",
    "def load_data(data_path):\n",
    "    try:\n",
    "        df = spark.read.format(\"delta\").load(data_path)\n",
    "        logging.info(f\"Data loaded from {data_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading data: {e}\")\n",
    "        return None\n",
    "\n",
    "# Aggregation functions\n",
    "def aggregate_by_time(df, time_col, agg_level):\n",
    "    if agg_level == 'date':\n",
    "        return df.groupBy(time_col, \"topic\", \"topic_name\") \\\n",
    "                 .agg(\n",
    "                     avg(\"score\").alias(\"average\"),\n",
    "                     expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "                     count(\"score\").alias(\"count\")\n",
    "                 ) \\\n",
    "                 .orderBy(col(time_col).desc()) \\\n",
    "                 .withColumnRenamed(time_col, agg_level) \n",
    "    else:\n",
    "        return df.withColumn(agg_level, date_trunc(agg_level, col(time_col))) \\\n",
    "                 .groupBy(agg_level, \"topic\", \"topic_name\") \\\n",
    "                 .agg(\n",
    "                     avg(\"score\").alias(\"average\"),\n",
    "                     expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "                     count(\"score\").alias(\"count\")\n",
    "                 ) \\\n",
    "                 .orderBy(col(agg_level).desc())\n",
    "\n",
    "\n",
    "def aggregate_total(df):\n",
    "    return df.groupBy( \"topic\", \"topic_name\").agg(\n",
    "        avg(\"score\").alias(\"average\"),\n",
    "        expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "        count(\"score\").alias(\"count\")\n",
    "    )\n",
    "\n",
    "# Export functions\n",
    "def export_to_parquet(df, export_path):\n",
    "    try:\n",
    "        df.coalesce(1).write.mode(\"overwrite\").parquet(export_path)\n",
    "        logging.info(f\"Data exported to {export_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error exporting data to {export_path}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d580449-26e9-4481-986b-4b7cf985170d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Main logic\n",
    "# Load the data\n",
    "data_path = \"s3a://topic-prediction/delta/reviews_predictions\"\n",
    "loaded_df = load_data(data_path)\n",
    "\n",
    "if loaded_df:\n",
    "    # Add the necessary transformations after loading the data\n",
    "    loaded_df = loaded_df.withColumn('topic_name', concat_ws('_', col('topic_name')))\n",
    "    loaded_df = loaded_df.withColumn(\"date\", to_date(col(\"review_timestamp\")))\n",
    "\n",
    "    # Aggregation by month with additional transformations\n",
    "    results_by_month_topic_df = aggregate_by_time(loaded_df, \"date\", \"month\")\n",
    "\n",
    "    # Define the window specification for month-to-month comparison\n",
    "    window_spec = Window.partitionBy(\"topic\").orderBy(\"month\")\n",
    "\n",
    "    # Calculate previous month's average, absolute difference, and percentage difference\n",
    "    results_by_month_topic_df = results_by_month_topic_df \\\n",
    "        .withColumn(\"prev_month_avg\", lag(\"average\", 1).over(window_spec)) \\\n",
    "        .withColumn(\"abs_diff\", col(\"average\") - col(\"prev_month_avg\")) \\\n",
    "        .withColumn(\"pct_diff\", ((col(\"average\") - col(\"prev_month_avg\")) / col(\"prev_month_avg\")) * 100) \\\n",
    "        .orderBy(col('topic_name').desc())\n",
    "\n",
    "        # Export the updated month topic aggregations\n",
    "    export_to_parquet(results_by_month_topic_df, f\"{EXPORT_PATH_BASE}/agg_by_month_topic\")\n",
    "\n",
    "        # Aggregation by week\n",
    "    results_by_week_topic_df = aggregate_by_time(loaded_df, \"date\", \"week\")\n",
    "    export_to_parquet(results_by_week_topic_df, f\"{EXPORT_PATH_BASE}/agg_by_week_topic\")\n",
    "\n",
    "        # Aggregation by date\n",
    "    results_by_date_topic_df = aggregate_by_time(loaded_df, \"date\", \"date\")\n",
    "    export_to_parquet(results_by_date_topic_df, f\"{EXPORT_PATH_BASE}/agg_by_date_topic\")\n",
    "\n",
    "        # Aggregation by total (for overall data)\n",
    "    total_aggregations_df = aggregate_total(loaded_df)\n",
    "    export_to_parquet(total_aggregations_df, f\"{EXPORT_PATH_BASE}/total_aggregations\")\n",
    "\n",
    "        # Export top topics by score\n",
    "    top_topics_by_score_df = loaded_df.groupBy(\"score\",\"topic\", \"topic_name\").agg(\n",
    "            count(\"review_timestamp\").alias(\"count\")\n",
    "    ).orderBy(col('score').asc_nulls_last(),col('count').desc())\n",
    "    export_to_parquet(top_topics_by_score_df, f\"{EXPORT_PATH_BASE}/top_topics_by_score\")\n",
    "\n",
    "        # Export scores count\n",
    "    scores_count_df = loaded_df.groupBy(\"score\").agg(\n",
    "            count(\"topic\").alias(\"count_topics\"),\n",
    "            count(\"topic_name\").alias(\"count_topic_name\")\n",
    "    ).orderBy(col('count_topics').desc())\n",
    "    export_to_parquet(scores_count_df, f\"{EXPORT_PATH_BASE}/scores_count\")\n",
    "\n",
    "        # Export raw data\n",
    "    export_to_parquet(loaded_df, f\"{EXPORT_PATH_BASE}/raw_data\")\n",
    "        \n",
    "        # Aggregation by week without topic (for overall weekly data)\n",
    "    agg_by_week_df = loaded_df.withColumn(\"week\", date_trunc(\"week\", col(\"date\"))) \\\n",
    "                                .groupBy(\"week\") \\\n",
    "                                .agg(\n",
    "                                    avg(\"score\").alias(\"average\"),\n",
    "                                    expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "                                    count(\"score\").alias(\"count\")\n",
    "                                ).orderBy(col(\"week\").desc())\n",
    "    export_to_parquet(agg_by_week_df, f\"{EXPORT_PATH_BASE}/agg_by_week\")\n",
    "\n",
    "        # Aggregation by month without topic (for overall monthly data)\n",
    "    agg_by_month_df = loaded_df.withColumn(\"month\", date_trunc(\"month\", col(\"date\"))) \\\n",
    "                                .groupBy(\"month\") \\\n",
    "                                .agg(\n",
    "                                    avg(\"score\").alias(\"average\"),\n",
    "                                    expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "                                    count(\"score\").alias(\"count\")\n",
    "                                ).orderBy(col(\"month\").desc())\n",
    "    export_to_parquet(agg_by_month_df, f\"{EXPORT_PATH_BASE}/agg_by_month\")\n",
    "\n",
    "        # Aggregation by date without topic (for overall daily data)\n",
    "    agg_by_date_df = loaded_df.withColumn(\"date\", date_trunc(\"day\", col(\"date\"))) \\\n",
    "                                .groupBy(\"date\") \\\n",
    "                                 .agg(\n",
    "                                    avg(\"score\").alias(\"average\"),\n",
    "                                    expr(\"percentile_approx(score, 0.5)\").alias(\"median\"),\n",
    "                                    count(\"score\").alias(\"count\")\n",
    "                                ).orderBy(col(\"date\").desc())\n",
    "    export_to_parquet(agg_by_date_df, f\"{EXPORT_PATH_BASE}/agg_by_date\")\n",
    "\n",
    "else:\n",
    "    logging.error(\"Data not loaded, skipping processing.\")\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pre_aggregate_gpt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
