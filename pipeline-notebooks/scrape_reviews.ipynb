{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "825906d3-00d9-4757-89a5-0fa504a2d6d2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from google_play_scraper import reviews, Sort\n",
    "import boto3\n",
    "import logging\n",
    "from io import StringIO\n",
    "import csv\n",
    "\n",
    "# Setup logging to display messages directly in the notebook\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b29ec3eb-3b5f-4d84-9bae-f6822df16d24",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configure AWS S3 credentials\n",
    "AWS_ACCESS_KEY_ID = dbutils.secrets.get(scope=\"s3_secrets\", key=\"AWS_ACCESS_KEY_ID\")\n",
    "AWS_SECRET_ACCESS_KEY = dbutils.secrets.get(scope=\"s3_secrets\", key=\"AWS_SECRET_ACCESS_KEY\")\n",
    "AWS_DEFAULT_REGION = dbutils.secrets.get(scope=\"s3_secrets\", key=\"AWS_DEFAULT_REGION\")\n",
    "\n",
    "# Create an S3 client using boto3\n",
    "s3_client = boto3.client(\n",
    "    service_name='s3',\n",
    "    region_name=AWS_DEFAULT_REGION,\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY\n",
    ")\n",
    "logging.info(\"Successfully created S3 client.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0f88057d-eba5-4f84-987b-f0e23144d2be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def scrape_reviews(product_id, date_filter):\n",
    "    logging.info(f\"Scraping reviews for product ID: {product_id}\")\n",
    "    result, _ = reviews(\n",
    "        product_id,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=Sort.NEWEST,\n",
    "        count=200  # Increased count to get more data\n",
    "    )\n",
    "\n",
    "    # Convert to DataFrame and filter by date\n",
    "    df = pd.DataFrame(result)\n",
    "    filtered_df = df[df['at'].dt.date == date_filter]\n",
    "    logging.info(f\"Scraped {len(filtered_df)} reviews for date {date_filter}.\")\n",
    "    return filtered_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef6fd8ca-7f17-4684-9a14-1b1021ace612",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "PRODUCT_ID = \"droom.sleepIfUCan\"\n",
    "BUCKET_NAME = 'topic-prediction'\n",
    "today = datetime.date.today()\n",
    "yesterday = today - datetime.timedelta(days=1)\n",
    "\n",
    "scraped_reviews_df = scrape_reviews(PRODUCT_ID, yesterday)\n",
    "\n",
    "\n",
    "# iterate over each column in the DataFrame\n",
    "for col in scraped_reviews_df.columns:\n",
    "    if scraped_reviews_df[col].dtype == object:\n",
    "        # remove newline characters by replacing them with nothing\n",
    "        scraped_reviews_df[col] = scraped_reviews_df[col].str.replace('\\n', '', regex=False)\n",
    "\n",
    "\n",
    "if not scraped_reviews_df.empty:\n",
    "    # create a unique file key with timestamp for S3\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')\n",
    "    file_key = f'raw_data/raw_data_{timestamp}.csv'\n",
    "\n",
    "    # upload to S3\n",
    "    csv_buffer = StringIO()\n",
    "    scraped_reviews_df.to_csv(csv_buffer, index=False)\n",
    "    s3_client.put_object(Bucket=BUCKET_NAME, Key=file_key, Body=csv_buffer.getvalue())\n",
    "    logging.info(f\"Uploaded file {file_key} to bucket {BUCKET_NAME}.\")\n",
    "else:\n",
    "    logging.warning(f\"No reviews found for the date: {yesterday}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "scrape_gpt",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
