{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f897ea7c-5c44-4573-a9c1-bbb8dee45120",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration and Imports\n",
    "import boto3\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# set up logging \n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4e6143a6-5208-4564-be4d-71921872f51c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Helper Function Definitions\n",
    "\n",
    "# Initialize the S3 client\n",
    "def get_s3_client():\n",
    "    \"\"\"\n",
    "    Initializes and returns an S3 client.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = boto3.client('s3')\n",
    "        logging.info(\"Successfully initialized S3 client.\")\n",
    "        return s3_client\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to create S3 client: {e}\")\n",
    "        raise\n",
    "\n",
    "# Get the latest file in a specified S3 folder\n",
    "def get_latest_s3_file(s3_client, bucket_name, folder_path):\n",
    "    \"\"\"\n",
    "    List and get the most recent file from an S3 folder.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = s3_client.list_objects_v2(Bucket=bucket_name, Prefix=folder_path)\n",
    "        files = [(obj['Key'], obj['LastModified']) for obj in response.get('Contents', []) if 'LastModified' in obj]\n",
    "        if not files:\n",
    "            logging.warning(f\"No files found in S3 bucket '{bucket_name}' with prefix '{folder_path}'.\")\n",
    "            return None\n",
    "        latest_file = sorted(files, key=lambda x: x[1], reverse=True)[0][0]\n",
    "        logging.info(f\"Latest file found: {latest_file}\")\n",
    "        return latest_file\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error retrieving the latest file: {e}\")\n",
    "        raise\n",
    "\n",
    "# Read data from a CSV file in S3 into a Spark DataFrame\n",
    "def read_csv_to_spark(s3_path, schema):\n",
    "    \"\"\"\n",
    "    Read a CSV file from S3 into a Spark DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = spark.read.csv(s3_path, header=True, schema=schema)\n",
    "        logging.info(f\"Successfully read data from: {s3_path}\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading CSV file from S3 path {s3_path}: {e}\")\n",
    "        raise\n",
    "\n",
    "# Process the DataFrame to filter and select required columns\n",
    "def process_reviews_dataframe(df):\n",
    "    \"\"\"\n",
    "    Filter out rows with null content and select necessary columns.\n",
    "    \"\"\"\n",
    "    filtered_df = df.filter(col(\"content\").isNotNull())\n",
    "    required_data_df = filtered_df.select('content', 'reviewCreatedVersion', 'score', 'at', 'reviewId') \\\n",
    "                                 .withColumnRenamed(\"at\", \"review_timestamp\") \\\n",
    "                                 .withColumnRenamed(\"reviewId\", \"review_id\")\n",
    "    logging.info(\"Filtered and selected required columns from the DataFrame.\")\n",
    "    return required_data_df\n",
    "\n",
    "# Save the processed DataFrame as a Delta table\n",
    "def save_as_delta_table(df, delta_table_path):\n",
    "    \"\"\"\n",
    "    Save the processed DataFrame as a Delta table.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df.write.format(\"delta\").mode(\"append\").save(delta_table_path)\n",
    "        logging.info(f\"Data successfully saved to Delta table at: {delta_table_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving DataFrame to Delta table: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7555ff44-6e53-4207-bf07-a4de39e0bbc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration Setup\n",
    "\n",
    "# Define constants for S3 bucket and paths\n",
    "BUCKET_NAME = 'topic-prediction'\n",
    "FOLDER_PATH = 'raw_data'\n",
    "DELTA_TABLE_PATH = \"/mnt/topic-prediction/delta/reviews\" \n",
    "\n",
    "# Define the schema for reading the CSV file\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"reviewId\", StringType(), True),\n",
    "    StructField(\"userName\", StringType(), True),\n",
    "    StructField(\"userImage\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"thumbsUpCount\", IntegerType(), True),\n",
    "    StructField(\"reviewCreatedVersion\", StringType(), True),\n",
    "    StructField(\"at\", TimestampType(), True),\n",
    "    StructField(\"replyContent\", StringType(), True),\n",
    "    StructField(\"repliedAt\", TimestampType(), True),\n",
    "    StructField(\"appVersion\", IntegerType(), True)\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189e3923-adf3-4a0d-a4b6-8c9667c2f6bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Main Workflow\n",
    "\n",
    "try:\n",
    "    #Initialize the S3 client\n",
    "    s3_client = get_s3_client()\n",
    "\n",
    "    #Retrieve the latest file from the specified folder\n",
    "    latest_file = get_latest_s3_file(s3_client, BUCKET_NAME, FOLDER_PATH)\n",
    "\n",
    "    #If a file is found, continue processing\n",
    "    if latest_file:\n",
    "        s3_path = f\"s3a://{BUCKET_NAME}/{latest_file}\" \n",
    "        raw_df = read_csv_to_spark(s3_path, SCHEMA)  \n",
    "        processed_df = process_reviews_dataframe(raw_df) \n",
    "        save_as_delta_table(processed_df, DELTA_TABLE_PATH)  \n",
    "\n",
    "        # Show a small sample for validation\n",
    "        display(processed_df.limit(5))\n",
    "    else:\n",
    "        logging.warning(\"No files found to process.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Data processing pipeline failed: {e}\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1052430559342292,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "s3->spark_delta_lake - gpt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
