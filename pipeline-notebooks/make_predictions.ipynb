{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30cce616-e2cd-4734-a91e-fb799deb4517",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Configuration and Imports\n",
    "from bertopic import BERTopic\n",
    "from huggingface_hub import login\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SparkSession\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType, ArrayType\n",
    "import os\n",
    "\n",
    "# Define the Delta table paths\n",
    "preprocessed_delta_table_path = \"/mnt/topic-prediction/delta/reviews/\"  # Table with preprocessed data\n",
    "predictions_delta_table_path = \"/mnt/topic-prediction/delta/reviews_predictions/\"  # Table to store predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93e8d8e4-90d0-4553-9798-b69ac96931d9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load preprocessed data from Delta table\n",
    "raw_df = spark.read.format(\"delta\").load(preprocessed_delta_table_path)\n",
    "\n",
    "# Check if the predictions Delta table exists\n",
    "predictions_table_exists = DeltaTable.isDeltaTable(spark, predictions_delta_table_path)\n",
    "\n",
    "if predictions_table_exists:\n",
    "    # Load predicted data from the Delta table if it exists\n",
    "    predicted_df = spark.read.format(\"delta\").load(predictions_delta_table_path).select(\"review_id\").distinct()\n",
    "    print(f\"Loaded existing predictions. Found {predicted_df.count()} distinct reviews already processed.\")\n",
    "else:\n",
    "    print(\"Predictions table does not exist. All data will be considered unprocessed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7fc5d288-7b6f-48c8-8114-6ad5a5913eab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if predictions_table_exists:\n",
    "    # Find unprocessed reviews using a left anti-join\n",
    "    unprocessed_ids = raw_df.select(\"review_id\").distinct().join(predicted_df, on=\"review_id\", how=\"left_anti\")\n",
    "    unprocessed_df = raw_df.join(unprocessed_ids, on=\"review_id\", how=\"inner\")\n",
    "else:\n",
    "    # All data is unprocessed\n",
    "    unprocessed_df = raw_df\n",
    "\n",
    "#remove null scores from unprocessed df\n",
    "unprocessed_df = unprocessed_df.filter(col(\"score\").isNotNull())\n",
    "\n",
    "\n",
    "# Display unprocessed data for quick inspection (optional)\n",
    "display(unprocessed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d8258bb-9b91-4842-8210-a72d77e5d791",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert to Pandas DataFrame for BERTopic processing\n",
    "unprocessed_pd = unprocessed_df.toPandas()\n",
    "\n",
    "# Check if there are no new reviews and exit notebook successfully\n",
    "if unprocessed_pd.shape[0] == 0:\n",
    "    dbutils.notebook.exit(\"No new reviews to process. Notebook finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69d90a5c-eab7-47d9-863c-acf5c497cb6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Retrieve HuggingFace token from secrets\n",
    "HUGGINGFACE_TOKEN = dbutils.secrets.get(scope=\"hugging_face\", key=\"login_token\")\n",
    "\n",
    "# Login to HuggingFace Hub\n",
    "login(HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Load the BERTopic model from HuggingFace\n",
    "loaded_model = BERTopic.load(\"DobreMihai/bertopic_ready_labeled\")\n",
    "\n",
    "print(\"Successfully loaded BERTopic model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f1aec822-ba74-4162-8c01-2e8a7dd4cde4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Apply BERTopic model on the review content\n",
    "docs = unprocessed_pd['content'].tolist()\n",
    "topics, probs = loaded_model.transform(docs)\n",
    "\n",
    "# Get topic representations\n",
    "topic_representations = loaded_model.get_topic_info()[['Topic', 'CustomName']]\n",
    "\n",
    "topic_representations = topic_representations.rename(columns={'Topic': 'topic', 'CustomName': 'topic_name'})\n",
    "\n",
    "# Add topics to the original DataFrame\n",
    "unprocessed_pd['topic'] = topics\n",
    "\n",
    "# Merge with topic representations\n",
    "predicted_pd = unprocessed_pd.merge(topic_representations, on='topic')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "746f328f-5adf-4a61-a486-253933bc47eb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Prepare to get back to spark\n",
    "\n",
    "# Convert float columns to integers\n",
    "predicted_pd['score'] = predicted_pd['score'].astype('Int64')\n",
    "\n",
    "\n",
    "# define schema for making it back to spark dataframe from pandas dataframe\n",
    "SCHEMA = StructType([\n",
    "    StructField(\"review_id\", StringType(), True),\n",
    "    StructField(\"content\", StringType(), True),\n",
    "    StructField(\"reviewCreatedVersion\", StringType(), True),\n",
    "    StructField(\"score\", IntegerType(), True),\n",
    "    StructField(\"review_timestamp\", TimestampType(), True),\n",
    "    StructField(\"topic\", IntegerType(), True),\n",
    "    StructField(\"topic_name\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b2490f0-5bf3-4487-93c6-ed77f2efba0a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#check final dataframe before going back to spark\n",
    "predicted_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fa1ce0a8-3fce-481b-8710-2c2ed528831b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Convert the Pandas DataFrame back to a Spark DataFrame\n",
    "predicted_df = spark.createDataFrame(predicted_pd, schema = SCHEMA)\n",
    "\n",
    "# Change topic label for uncategorised reviews to \"Uncategorised\"\n",
    "predicted_df = predicted_df.na.replace('-1_be_it_the_to', 'Uncategorised', 'topic_name')\n",
    "\n",
    "# Save the predictions to the Delta table, creating it if it does not exist\n",
    "predicted_df.write.format(\"delta\").mode(\"append\").save(predictions_delta_table_path)\n",
    "print(f\"Successfully saved predictions to Delta table: {predictions_delta_table_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2812870442166573,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "make_predictions_bertopic - gpt",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
